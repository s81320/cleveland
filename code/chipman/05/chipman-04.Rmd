---
title: "Chipman, George, McCulloch 1998"
author: 'Silke Meiner'
date: '7-July-2021'
output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: 2
---

note: The working directory is not set correctly.

result: We generate diverse sub-forests through cutoffs for dissimilarity of trees to be added. We set cutoff values such that the resulting sub-forest is around 10% of the full forest (50 , 500). This results in good accuracy ratios (1 and greater, thus improving the forests accuracy). 

Clustering does not help, it always reduces accuracy (this may be different when starting with small cutoff values, creating initially large sub-forests. Clustering larger forests may yield better results than clustering already small forests. Which is basically just removing a few trees. Which should have its own criteria for removal (cf feature selection, forward or backward)).

We have not considered quality of the trees we select into the sub-forest. There is no measure of quality (like a high accuracy) due to lack of robustness (when working with 2 sets of unseen data there is only small correlation of the accuracy on each set).

In this document we do not loop over different data splits and different forests. We only work with one fixed forest. This document is for explaining the chipman approach, suggesting it yields good results. The script dataEval-chipman.R does the looping over different data splits and different (ranger) random forests.

```{r}
rm(list=ls())
```

Required libraries.
```{r}
# install.packages('ranger')
library(ranger)
library(caret)
library(e1071)
library(cluster)
library(dplyr)
```


Source code.
```{r}
setwd("~/Documents/ds/sem4/thesis/cleveland-01") # this should not be necessary
source('code/source/distance-matrices-03-scaled01.R') # my own code
source('code/source/subforest.R') # constructors for a sub-forest (class ranger.forest) and its hull (class ranger)
source('code/source/chipman.R')
source('code/source/helper-functions.R')
```

read / load
* the cleveland data
* stratified data split: train, val and test
* a random forest of class ranger
* distance matrices from 4 metrices (d0,d1,d2 of Banerjee, sb: Shannon-Banks metric in Chipman paper)
```{r}
#setwd("~/Documents/ds/sem4/thesis/cleveland-01")
load('data/doc-500trees-10rep-5maxDepth.rda') # adds df (cleveland data set) and docN (10 times: datasplit, RF, distance matrices)
N <- length(docN)

train<-docN[[1]]$train
val<-docN[[1]]$val
test<-docN[[1]]$test
rg<-docN[[1]]$ranger
dm<-docN[[1]]$distMatrices
```

Looking at the accuracy of the full forest on the validation and test set tells us what to expect when running a model that performed well on the validation set on the test set: It will have a tendency to perform similar to the full forest :)

```{r}
accff<-apsf(rg$forest, 1:rg$num.trees , df[val,]) # accuracy (of) predictions (infered by) sub-forest
accff.test<-apsf(rg$forest, 1:rg$num.trees , df[test,])

accff ; accff.test
```

How can we use these distances of trees to select representative trees from the forest? We will follow the Chipman, George, McCulloch paper of 1998.

plots in Chipman: 
* mds: the trees of the forest mapped in 2 dim
* atp: added tree plot: starting with tree 1 (not in the plot) add trees to an increasing set of trees (the subforest). The plot shows for each added tree (starting with tree 2) the distances to the trees already selected.
* ctp: closest tree plot which shows only the the tree(s) the added tree is closest to.

```{r}
m<-'sb'
s<-30
dm[['d0']][1:s,1:s]
mds(dm[[m]][1:s,1:s],xylim=T, main=paste('mds',m))
atp(dm[[m]][1:s,1:s], addLabels = T)
ctdata(dm[[m]][1:s,1:s],3)
ctplot(dm[[m]][1:s,1:s],0.18)
dm[[m]][4,1:s] ; dm[[m]][5,1:s]
```

This will be our visualisation for the accuracy ratios of the set of selected trees (the sub-forest of diverse trees) and the further reduced sub-forest (of cluster medoids).
```{r}
plot2<- function(doc2){
  x.grid<-1:nrow(doc2)

  # cover the full range of accuracy ratios for the sub-forest of diverse trees and its clustered versions medoids
  ylim<-c(min(c(doc2$accRatio.sf,doc2$accRatio.sf.pam.ff)),
        max(c(1,doc2$accRatio.sf,doc2$accRatio.sf.pam.ff)))

  plot(x.grid
     , y=doc2$accRatio.sf
     , ylim=ylim
     , xlab='cutoff'
     , ylab='accuracy ratios relative to full forest'
     , main='subforest of diverse trees, original and clustered'
     , xaxt='n')

  axis(1, at =1:nrow(doc2), labels=doc2$cutoff)
  
  points(x.grid, doc2$accRatio.sf.pam.ff, col='blue')

  abline(h=1,col='grey')

  text(x=x.grid # this should plot the cluster sizes next to the blue accuracy dots
     , y=doc2$accRatio.sf.pam.ff
     , pos=4
     , labels=doc2$size.sf.pam)

  legend('bottomright'
       , legend=c('subforest','clustered sf')
       , col=c('black','blue')
       , pch='o'
       , cex=0.7)
}
```

```{r}
eval.on.test<-function(metric,cutoff,k=NA, doc=doc){
  dm2<-doc[[metric]]

  div.added.trees <- ctdata(dm2,cutoff)$selectTrees
  size.sf<-length(div.added.trees)
  print(paste('The selected sub-forest is of size',size.sf,'.'))
  selected.trees <- c(1,div.added.trees)
  
  if(!is.na(k)){
    pam.obj <- cluster::pam(dm2[selected.trees,selected.trees]
                          , k=k
                          , diss=TRUE
                          , medoids='random'
                          , nstart = 5)
  
    selected.trees <- pam.obj$medoids # update selected trees to cluster centers of selected trees
  
  }
  return(apsf(rg$forest, selected.trees, df[test,]))
}

```

# d0 metric
```{r fig.height=15, fig.width=5}
metric<-'d0'
par(mfrow = c(3, 1))
mds(dm[[metric]],xylim=T, main='mds d0')
mds(dm[[metric]],xylim=F, main='mds d0 zoomed in')
ctplot(dm[[metric]])
par(mfrow = c(1, 1))
```

Clustering in multiples of 4 seems right, since there are 4 obvious clusters. However, it does not perform better in terms of accuracy ratio of the clustered sub-forest. 

For comparison we cluster the same way as for the other metrices, with the primes.
```{r}
metric<-'d0'
cutoffs<-c(0.05,0.15,0.25,0.35)
cluster.k<-c(3,5,7,11,13)
#cluster.k<- c(4,8,12)
#cluster.k<-c(3,4,5,7,8,11,12,13)
set.seed(1871)
cAR<-chipmanAccRatios(metric, cutoffs, cluster.k, rg$forest, dm[[metric]], df[val,]) # c hipman A ccuracy R atios
cAR
```


```{r}
plot2(cAR)
```

## Evaluation on test set

There is no choice for the cutoff. We decide against clustering.

```{r}
metric<-'d0'
cutoff<-0.25

at<-eval.on.test(metric, cutoff, doc=dm)
at ; at / accff.test
```

This is a good result. And agrees with the performance on the validation set.

## Hypothesis test (on val data)
```{r}
t.test(cAR$accRatio.sf.pam.ff, mu=0.95 , alternative='greater')
```
We start out conservatively and miserable: H0: the accuracy ratio is below 0.95. On a 5% level we can reject this hypothesis. What a relief!

```{r}
boxplot(cbind(cAR$accRatio.sf, cAR$accRatio.sf.pam.ff) , main='accuracy ratios for unclustered and clustered sub-forests')
```

# d1 metric

```{r fig.height=15, fig.width=5}
metric<-'d1'
par(mfrow = c(3, 1))
mds(dm[[metric]], xylim=T , main='mds d1')
mds(dm[[metric]], xylim=F , main='mds d1, zoomed in')
ctplot(dm[[metric]])
par(mfrow = c(1, 1))
```

```{r}
cutoffs<-c(0.25,0.3,0.35)
cluster.k<-c(3,5,7,11,13)
set.seed(1870)
cAR<-chipmanAccRatios(metric, cutoffs, cluster.k, rg$forest, dm[[metric]], df[val,])
cAR
```

```{r}
plot2(cAR)
```
## Evaluation on test set

We choose cutoff 0.35 for its high accuracy ratio (not the highest, though) and its small size for the resulting sub-forest.
```{r}
metric='d1'
cutoff=0.35

at<-eval.on.test(metric, cutoff, doc=dm) # a_ccuracy t_est set
at ; at / accff.test
```

Good result, too, accuracy ratio 1.

## Hypothesis test (on val data)
```{r}
t.test(cAR$accRatio.sf , mu=0.95, alternative = 'greater') # mean reliably >1 ? 
t.test(cAR$accRatio.sf.pam.ff , mu=0.95 , alternative = 'greater') # mean (accRatio) > 1 ? CI

#which method is better?
t.test(cAR$accRatio.sf - cAR$accRatio.sf.pam.ff, mu=0 , alternative='greater') # positive CI => unclustered is better
```
```{r}
boxplot(cbind(cAR$accRatio.sf, cAR$accRatio.sf.pam.ff) , main='accuracy ratios for unclustered and clustered sub-forests')
```


# d2 metric
```{r fig.height=15, fig.width=5}
metric<-'d2'
par(mfrow = c(3, 1))
mds(dm[[metric]],xylim=T, main='mds d2')
mds(dm[[metric]],xylim=F, main='mds d2 zoomed in')
ctplot(dm[[metric]] )
par(mfrow = c(1, 1))
```


```{r}
metric<-'d2'
cutoffs<-c(0.24,0.25,0.26,0.27,0.28)
cluster.k<-c(5,7,11,13)
set.seed(1870)
cAR<-chipmanAccRatios(metric, cutoffs, cluster.k, rg$forest, dm[[metric]], df[val,])
cAR
```

```{r}
plot2(cAR)
```

## Evaluation on test set

We choose the optimal cutoff of 0.28 because of its performance (hoghest accuracy ratio) and the resulting sub-forest's size (smalles). Regarding the performance on the validation set there was no linear trend observable, so we assume it is rather variable.
```{r}
metric='d2'
cutoff=0.28

at<-eval.on.test(metric, cutoff, doc=dm)
at ; at / accff.test
```
The accuracy ratio on the test set  is below 1 and even below 0.95. This is not a good result. We had assumed high variability of the resulting accuracy ratio. Seems we got no luck this time.

## Hypothesis test (on val data)

```{r}
t.test(cAR$accRatio.sf , mu=0.95, alternative = 'greater') # mean reliably >1 ? 
t.test(cAR$accRatio.sf.pam.ff , mu=0.95 , alternative = 'greater') # mean (accRatio) > 1 ? CI

#which method is better?
t.test(cAR$accRatio.sf - cAR$accRatio.sf.pam.ff, mu=0 , alternative='greater') # positive CI => unclustered is better
```
```{r}
boxplot(cbind(cAR$accRatio.sf, cAR$accRatio.sf.pam.ff) , main='accuracy ratios for unclustered and clustered sub-forests')
```
# shannon banks metric

```{r fig.height=15, fig.width=5}
metric<-'sb'
par(mfrow = c(3, 1))
mds(dm[[metric]], xylim=T , main='mds shannon banks')
mds(dm[[metric]], xylim=F , main='mds shannon banks, zoomed in')
ctplot(dm[[metric]])
par(mfrow = c(1, 1))
```


```{r}
metric<-'sb'
cutoffs<-c(0.65,0.7,0.75)
cluster.k<-c(5,7,11,13)
# cluster.k<-c(5,10,15,20,25)
set.seed(1870)
cAR<-chipmanAccRatios(metric, cutoffs, cluster.k, rg$forest, dm[[metric]], df[val,])
cAR
```
```{r}
plot2(cAR)
```
## Evaluation on test set

Optimal parameters are a large cutoff and no clustering: The accuracy ratio is above 1 on the validation set.


```{r}
metric='sb'
cutoff=0.75
at<-eval.on.test(metric, cutoff, doc=dm)
at ; at / accff.test
```

Again, bad luck on the test set?

## Hypothesis test (on val data)

```{r}
t.test(cAR$accRatio.sf , mu=0.95, alternative = 'greater') # mean reliably >1 ? 
t.test(cAR$accRatio.sf.pam.ff , mu=0.95 , alternative = 'greater') # mean (accRatio) > 1 ? CI

#which method is better?
t.test(cAR$accRatio.sf - cAR$accRatio.sf.pam.ff, mu=0 , alternative='greater') # positive CI => unclustered is better
```

```{r}
boxplot(cbind(cAR$accRatio.sf, cAR$accRatio.sf.pam.ff) , main='accuracy ratios for unclustered and clustered sub-forests')
```

